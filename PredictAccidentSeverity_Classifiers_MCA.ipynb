{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Project\n",
    "\n",
    "This project is to predict how various features like geographic location, weather conditions, type of vehicles etc. can be used to predict the severity of an accident. In other words, the project goal is to build a classification model for road accidents in UK to predict the Accident Severity.\n",
    "\n",
    "##### This notebook demonstrates MCA dimension reduction and further model classifiers GaussianNB, poly SVM and Shallow Neural Network classifiers. SVC algorithm yields best results overall.\n",
    "\n",
    "Data: Road accidents in UK between 2010 and 2014  \n",
    "Link: https://www.kaggle.com/stefanoleone992/adm-project-road-accidents-in-uk  \n",
    "Dataset: 75550 x 33  \n",
    "Response: 0 : Slight and 1 : Fatal-Serious  \n",
    "\n",
    "A classifier is built using 3 models- Naive Bayes, SVM and Neural Network for the task.\n",
    "There are 2 parts to the process.\n",
    "1. Data exploration, relevant teachniques from feature engineering, dimension reduction and feature selection are explored and implemented.\n",
    "2. Data modeling based on Part 1 analysis, comparison of scores and final procedure selection for documentation. \n",
    "\n",
    "Metrics : Accuracy\n",
    "\n",
    "### Overall Approach\n",
    "We explored various data preparation and machine learning techniques:\n",
    "##### 1.Data level\n",
    "The various approaches to prepare data explored are explained in brief as we go along.\n",
    "In general, amongst the various approaches tried, we talk about 2 approaches:\n",
    "1. **Feature selection with Random Forest** at different variabilities (90%, 95%, 98%), with 95% yielding best accuracy scores overall.\n",
    "2. **Dimension reduction technique with MCA** since the dataset is predominantly categorical with various n_components(5 to 22) with 22 yielding best accuracy scores overall. \n",
    "\n",
    "Both methods yield comparable results and we demonstrate method 2 here for the project which gives little better accuracy score.\n",
    "\n",
    "##### 2.Classifier level\n",
    "1. Since the dataset is large, stratified sampling technique is used with 50-50 train test split. \n",
    "2. Various relevant algorithms in classifiers paired with feature selection/reduction techniques were explored for instance, Gaussian, Categorical, Multinomial and Mixed for Naive Bayes, Linear,Poly and Rbf kernel for SVC and shallow and deep network for neural network.\n",
    "3. Further, data modeling is done with hyperparameter tuning through k-fold grid search CV for all classifiers and accuracy score is reported on test set.\n",
    "4. Threshold tuning(range 0.4 to 0.6) and found 0.55 threshold yielding best results for majority classifiers. \n",
    "\n",
    "**Notebook By:  \n",
    "Swati Kohli  \n",
    "Poonam Patil**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Relevant Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PreProcessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import calendar\n",
    "\n",
    "# Dimension Reduction\n",
    "import prince\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Data Modeling\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75550, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Region</th>\n",
       "      <th>Urban_or_Rural_Area</th>\n",
       "      <th>X1st_Road_Class</th>\n",
       "      <th>Driver_IMD_Decile</th>\n",
       "      <th>Speed_limit</th>\n",
       "      <th>Road_Type</th>\n",
       "      <th>Road_Surface_Conditions</th>\n",
       "      <th>...</th>\n",
       "      <th>Junction_Detail</th>\n",
       "      <th>Junction_Location</th>\n",
       "      <th>X1st_Point_of_Impact</th>\n",
       "      <th>Driver_Journey_Purpose</th>\n",
       "      <th>Engine_CC</th>\n",
       "      <th>Propulsion_Code</th>\n",
       "      <th>Vehicle_Make</th>\n",
       "      <th>Vehicle_Category</th>\n",
       "      <th>Vehicle_Manoeuvre</th>\n",
       "      <th>Accident_Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51.495653</td>\n",
       "      <td>-0.179097</td>\n",
       "      <td>London</td>\n",
       "      <td>Urban</td>\n",
       "      <td>C</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>Dry</td>\n",
       "      <td>...</td>\n",
       "      <td>Not at junction or within 20 metres</td>\n",
       "      <td>Not at or within 20 metres of junction</td>\n",
       "      <td>Front</td>\n",
       "      <td>Other/Not known</td>\n",
       "      <td>1781</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Audi</td>\n",
       "      <td>Car</td>\n",
       "      <td>Going ahead</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>51.499635</td>\n",
       "      <td>-0.209915</td>\n",
       "      <td>London</td>\n",
       "      <td>Urban</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>Dry</td>\n",
       "      <td>...</td>\n",
       "      <td>More than 4 arms (not roundabout)</td>\n",
       "      <td>Mid Junction - on roundabout or on main road</td>\n",
       "      <td>Offside</td>\n",
       "      <td>Other/Not known</td>\n",
       "      <td>2987</td>\n",
       "      <td>Heavy oil</td>\n",
       "      <td>Mercedes</td>\n",
       "      <td>Car</td>\n",
       "      <td>Waiting to go</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51.492515</td>\n",
       "      <td>-0.168130</td>\n",
       "      <td>London</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Unclassified</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>Dry</td>\n",
       "      <td>...</td>\n",
       "      <td>Crossroads</td>\n",
       "      <td>Mid Junction - on roundabout or on main road</td>\n",
       "      <td>Front</td>\n",
       "      <td>Journey as part of work</td>\n",
       "      <td>998</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Nissan</td>\n",
       "      <td>Car</td>\n",
       "      <td>Going ahead</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>51.504784</td>\n",
       "      <td>-0.193863</td>\n",
       "      <td>London</td>\n",
       "      <td>Urban</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>Dry</td>\n",
       "      <td>...</td>\n",
       "      <td>T or staggered junction</td>\n",
       "      <td>Mid Junction - on roundabout or on main road</td>\n",
       "      <td>Offside</td>\n",
       "      <td>Journey as part of work</td>\n",
       "      <td>2179</td>\n",
       "      <td>Heavy oil</td>\n",
       "      <td>Citroen</td>\n",
       "      <td>Van</td>\n",
       "      <td>Turning right</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>51.522072</td>\n",
       "      <td>-0.212927</td>\n",
       "      <td>London</td>\n",
       "      <td>Urban</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>Single carriageway</td>\n",
       "      <td>Wet or damp</td>\n",
       "      <td>...</td>\n",
       "      <td>T or staggered junction</td>\n",
       "      <td>Approaching junction or waiting/parked at junc...</td>\n",
       "      <td>Nearside</td>\n",
       "      <td>Journey as part of work</td>\n",
       "      <td>2198</td>\n",
       "      <td>Heavy oil</td>\n",
       "      <td>Ford</td>\n",
       "      <td>Van</td>\n",
       "      <td>Overtaking</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accident_Index   Latitude  Longitude  Region Urban_or_Rural_Area  \\\n",
       "0               1  51.495653  -0.179097  London               Urban   \n",
       "1               2  51.499635  -0.209915  London               Urban   \n",
       "2               3  51.492515  -0.168130  London               Urban   \n",
       "3               4  51.504784  -0.193863  London               Urban   \n",
       "4               5  51.522072  -0.212927  London               Urban   \n",
       "\n",
       "  X1st_Road_Class  Driver_IMD_Decile  Speed_limit           Road_Type  \\\n",
       "0               C                  7           30  Single carriageway   \n",
       "1               A                  3           30  Single carriageway   \n",
       "2    Unclassified                  5           30  Single carriageway   \n",
       "3               A                  2           30  Single carriageway   \n",
       "4               B                  3           30  Single carriageway   \n",
       "\n",
       "  Road_Surface_Conditions  ...                      Junction_Detail  \\\n",
       "0                     Dry  ...  Not at junction or within 20 metres   \n",
       "1                     Dry  ...    More than 4 arms (not roundabout)   \n",
       "2                     Dry  ...                           Crossroads   \n",
       "3                     Dry  ...              T or staggered junction   \n",
       "4             Wet or damp  ...              T or staggered junction   \n",
       "\n",
       "                                   Junction_Location X1st_Point_of_Impact  \\\n",
       "0             Not at or within 20 metres of junction                Front   \n",
       "1       Mid Junction - on roundabout or on main road              Offside   \n",
       "2       Mid Junction - on roundabout or on main road                Front   \n",
       "3       Mid Junction - on roundabout or on main road              Offside   \n",
       "4  Approaching junction or waiting/parked at junc...             Nearside   \n",
       "\n",
       "    Driver_Journey_Purpose  Engine_CC  Propulsion_Code  Vehicle_Make  \\\n",
       "0          Other/Not known       1781           Petrol          Audi   \n",
       "1          Other/Not known       2987        Heavy oil      Mercedes   \n",
       "2  Journey as part of work        998           Petrol        Nissan   \n",
       "3  Journey as part of work       2179        Heavy oil       Citroen   \n",
       "4  Journey as part of work       2198        Heavy oil          Ford   \n",
       "\n",
       "   Vehicle_Category  Vehicle_Manoeuvre  Accident_Severity  \n",
       "0               Car        Going ahead             Slight  \n",
       "1               Car      Waiting to go             Slight  \n",
       "2               Car        Going ahead             Slight  \n",
       "3               Van      Turning right             Slight  \n",
       "4               Van         Overtaking             Slight  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "road_accidents = pd.read_csv('RoadAccident.csv')\n",
    "print(road_accidents.shape)\n",
    "road_accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56705\n",
       "1    18845\n",
       "Name: Accident_Severity, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Binary encode target variable Accident_Severity\n",
    "road_accidents['Accident_Severity'] = road_accidents['Accident_Severity'].map({'Fatal_Serious': 1, 'Slight': 0})\n",
    "road_accidents['Accident_Severity'].value_counts() # Find distribution of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation: There is a large class imbalance in the dataset with class 0 records approximately three times the class 1 records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accident_Index             0\n",
       "Latitude                   0\n",
       "Longitude                  0\n",
       "Region                     0\n",
       "Urban_or_Rural_Area        0\n",
       "X1st_Road_Class            0\n",
       "Driver_IMD_Decile          0\n",
       "Speed_limit                0\n",
       "Road_Type                  0\n",
       "Road_Surface_Conditions    0\n",
       "Weather                    0\n",
       "High_Wind                  0\n",
       "Lights                     0\n",
       "Datetime                   0\n",
       "Year                       0\n",
       "Season                     0\n",
       "Month_of_Year              0\n",
       "Day_of_Month               0\n",
       "Day_of_Week                0\n",
       "Hour_of_Day                0\n",
       "Number_of_Vehicles         0\n",
       "Age_of_Driver              0\n",
       "Age_of_Vehicle             0\n",
       "Junction_Detail            0\n",
       "Junction_Location          0\n",
       "X1st_Point_of_Impact       0\n",
       "Driver_Journey_Purpose     0\n",
       "Engine_CC                  0\n",
       "Propulsion_Code            0\n",
       "Vehicle_Make               0\n",
       "Vehicle_Category           0\n",
       "Vehicle_Manoeuvre          0\n",
       "Accident_Severity          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing/NaN values\n",
    "road_accidents.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: No missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "## Feature engineering\n",
    "Diagnose features from the dataset and convert to appropriate categorical format.  \n",
    "1. Three numerical features in the dataset are converted to categorical- Season, Month_of_Year and Day_of_Week.   \n",
    "2. Further three more features-Age_of_Vehicle,Hour_of_Day and Engine_CC are transformed into categorical by binning. \n",
    "3. Unnecessary columns Latitude,Longitude, Datetime and Accident_Index are dropped since the locational and temporal features are engineered from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Season, Month_of_Year and Day_of_Week are categorical features** though they are having numerical value. Hence convert it to categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multinomial features to categorical by converting to datatype string\n",
    "road_accidents['Season'] = road_accidents['Season'].apply(str)\n",
    "road_accidents['Month_of_Year'] = road_accidents['Month_of_Year'].apply(str)\n",
    "road_accidents['Day_of_Week'] = road_accidents['Day_of_Week'].apply(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create categorical feature from Age_of_Vehicle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_accidents['Age_of_Vehicle_cat'] = pd.qcut(road_accidents['Age_of_Vehicle'], \n",
    "                                               [0,0.25,0.5,0.75,1], labels = ['new','average', 'old', 'very_old'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create categorical feature from Hour_of_Day predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_accidents['Hour_of_Day_cat'] = pd.qcut(road_accidents['Hour_of_Day'], \n",
    "        [0,0.25,0.5,0.75,1], labels=['early_morning', 'morning', 'afternoon', 'night'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create categorical feature from Engine_CC predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22320241388>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD5CAYAAADLL+UrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAATM0lEQVR4nO3df6zd9X3f8ecLx1A202DKDfJsg2nracGZ4sCdQ8ekUdIaw1SZTGHFUoMXRXMrgdZI7VSnmuT8GBqR1qAxUSRXuDFVXYTSZLESt9SDZFE0JWBnDsGQiFvCj1tbcFMTEkaGwXvvj/N1ezD3x7nX1+fa/jwf0tH5nvf38/1+P19kXud7P+dzzjdVhSSpDecsdAckScNj6EtSQwx9SWqIoS9JDTH0Jakh71joDkzn4osvrlWrVi10NyTpjLJ///4fVtXIZOtO69BftWoV+/btW+huSNIZJclzU61zeEeSGmLoS1JDDH1JaoihL0kNMfQlqSEzhn6Sn0nyaJLvJDmY5JNd/XNJfpDkQPdY29WT5O4kY0keT3Jl3742J3m6e2w+daclnTpJ3vaQzhSDTNl8Hbiuql5Nshj4RpK/6Nb9h6r6/AntbwBWd4/3A/cC709yEbANGAUK2J9kd1W9PB8nIg3DVAGfBH+xVmeCGa/0q+fV7uXi7jHdv+6NwP3ddt8ELkyyDLge2FtVR7qg3wtsOLnuS5JmY6Ax/SSLkhwAXqIX3N/qVt3RDeHcleS8rrYceKFv8/GuNlX9xGNtSbIvyb6JiYlZno40HIsXL37Ls3SmGCj0q+pYVa0FVgDrkrwH+DjwT4B/BlwE/F7XfLK/f2ua+onH2l5Vo1U1OjIy6beIpQX3xhtvvOVZOlPMavZOVf0I+BqwoaoOd0M4rwN/DKzrmo0DK/s2WwEcmqYunZGuuuqqhe6CNGuDzN4ZSXJht3w+8CvA97pxetL7ZOsm4Iluk93Ard0snquBV6rqMPAQsD7J0iRLgfVdTToj7d+/f6G7IM3aILN3lgE7kyyi9ybxYFV9OckjSUboDdscAH6ra78HuBEYA14DPgJQVUeSfBp4rGv3qao6Mn+nIkmaSU7naWajo6Plr2zqdDLdnPzT+f8ltSXJ/qoanWyd38iVpIYY+pLUEENfmqUlS5ZQVX/3WLJkyUJ3SRrYaX3nLOl09Oqrr/p7OzpjeaUvSQ0x9KU5eOSRRzh69CiPPPLIQndFmhWHd6Q5uO666xa6C9KceKUvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGvjRL55577rSvpdOZoS/N0tGjR1mzZg3PPfcca9as4ejRowvdJWlghr40C7fffjsABw8e5LLLLuPgwYNvqUunuxlvjJ7kZ4CvA+fR+1XOz1fVtiSXAw8AFwHfBj5cVUeTnAfcD1wF/C3w61X1bLevjwMfBY4B/76qHpru2N4YXcMyrJuiePN0DcPJ3hj9deC6qnovsBbYkORq4DPAXVW1GniZXpjTPb9cVb8I3NW1I8kVwC3AGmAD8IdJFs39tKT503/7w0Efl/3el2e9jbTQZgz96nm1e7m4exRwHfD5rr4TuKlb3ti9plv/gfQuozYCD1TV61X1A2AMWDcvZyFJGshAY/pJFiU5ALwE7AX+GvhRVb3ZNRkHlnfLy4EXALr1rwA/11+fZJv+Y21Jsi/JvomJidmfkSRpSgOFflUdq6q1wAp6V+fvnqxZ9zzZ4GhNUz/xWNurarSqRkdGRgbpniRpQLOavVNVPwK+BlwNXJjk+O0WVwCHuuVxYCVAt/6dwJH++iTbSJKGYMbQTzKS5MJu+XzgV4CngK8CH+qabQa+1C3v7l7TrX+kep9g7QZuSXJeN/NnNfDofJ2IJGlmg9wYfRmws5tpcw7wYFV9OcmTwANJ/hPwv4H7uvb3AX+SZIzeFf4tAFV1MMmDwJPAm8BtVXVsfk9HkjSdGUO/qh4H3jdJ/RkmmX1TVf8XuHmKfd0B3DH7bkqS5oPfyJWkhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaMmPoJ1mZ5KtJnkpyMMlvd/VPJPmbJAe6x41923w8yViS7ye5vq++oauNJdl6ak5JkjSVGW+MDrwJ/E5VfTvJBcD+JHu7dXdV1X/pb5zkCuAWYA3wj4D/keQfd6vvAX4VGAceS7K7qp6cjxORJM1sxtCvqsPA4W75J0meApZPs8lG4IGqeh34QZIxYF23bqyqngFI8kDX1tCXpCGZ1Zh+klXA+4BvdaXbkzyeZEeSpV1tOfBC32bjXW2q+onH2JJkX5J9ExMTs+meJGkGA4d+kiXAnwMfq6ofA/cCvwCspfeXwB8cbzrJ5jVN/a2Fqu1VNVpVoyMjI4N2T5I0gEHG9EmymF7g/2lVfQGgql7sW/9HwJe7l+PAyr7NVwCHuuWp6pKkIRhk9k6A+4CnquqzffVlfc0+CDzRLe8GbklyXpLLgdXAo8BjwOoklyc5l96Hvbvn5zQkSYMY5Er/GuDDwHeTHOhqvw9sSrKW3hDNs8BvAlTVwSQP0vuA9k3gtqo6BpDkduAhYBGwo6oOzuO5SJJmMMjsnW8w+Xj8nmm2uQO4Y5L6num2kySdWn4jV5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDZkx9JOsTPLVJE8lOZjkt7v6RUn2Jnm6e17a1ZPk7iRjSR5PcmXfvjZ37Z9OsvnUnZYkaTKDXOm/CfxOVb0buBq4LckVwFbg4apaDTzcvQa4AVjdPbYA90LvTQLYBrwfWAdsO/5GIUkajhlDv6oOV9W3u+WfAE8By4GNwM6u2U7gpm55I3B/9XwTuDDJMuB6YG9VHamql4G9wIZ5PRtJ0rRmNaafZBXwPuBbwCVVdRh6bwzAu7pmy4EX+jYb72pT1U88xpYk+5Lsm5iYmE33JEkzeMegDZMsAf4c+FhV/TjJlE0nqdU09bcWqrYD2wFGR0fftl4axHs/+Ve88tM3TvlxVm39yind/zvPX8x3tq0/pcdQWwYK/SSL6QX+n1bVF7ryi0mWVdXhbvjmpa4+Dqzs23wFcKirX3tC/Wtz77o0tVd++gbP3vmvFrobJ+1Uv6moPYPM3glwH/BUVX22b9Vu4PgMnM3Al/rqt3azeK4GXumGfx4C1idZ2n2Au76rSZKGZJAr/WuADwPfTXKgq/0+cCfwYJKPAs8DN3fr9gA3AmPAa8BHAKrqSJJPA4917T5VVUfm5SwkSQOZMfSr6htMPh4P8IFJ2hdw2xT72gHsmE0HJUnzx2/kSlJDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMGujG6dKa54N1b+ac7ty50N07aBe8GOPNv8K7Th6Gvs9JPnrqTZ+8888Ny1davLHQXdJaZcXgnyY4kLyV5oq/2iSR/k+RA97ixb93Hk4wl+X6S6/vqG7raWJIz/xJMks5Ag4zpfw7YMEn9rqpa2z32ACS5ArgFWNNt84dJFiVZBNwD3ABcAWzq2kqShmjG4Z2q+nqSVQPubyPwQFW9DvwgyRiwrls3VlXPACR5oGv75Kx7LEmas5OZvXN7kse74Z+lXW058EJfm/GuNlVdkjREcw39e4FfANYCh4E/6OqZpG1NU3+bJFuS7Euyb2JiYo7dkyRNZk6hX1UvVtWxqvp/wB/x90M448DKvqYrgEPT1Cfb9/aqGq2q0ZGRkbl0T5I0hTmFfpJlfS8/CByf2bMbuCXJeUkuB1YDjwKPAauTXJ7kXHof9u6ee7clSXMx4we5Sf4MuBa4OMk4sA24NslaekM0zwK/CVBVB5M8SO8D2jeB26rqWLef24GHgEXAjqo6OO9nI0ma1iCzdzZNUr5vmvZ3AHdMUt8D7JlV7yRJ88rf3pGkhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIZ4u0Sdtc6GWw2+8/zFC90FnWUMfZ2VhnF/3FVbv3JW3IdXbXF4R5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGjJj6CfZkeSlJE/01S5KsjfJ093z0q6eJHcnGUvyeJIr+7bZ3LV/OsnmU3M6kqTpDHKl/zlgwwm1rcDDVbUaeLh7DXADsLp7bAHuhd6bBLANeD+wDth2/I1CkjQ8M4Z+VX0dOHJCeSOws1veCdzUV7+/er4JXJhkGXA9sLeqjlTVy8Be3v5GIkk6xeY6pn9JVR0G6J7f1dWXAy/0tRvvalPV3ybJliT7kuybmJiYY/ckSZOZ7w9yM0mtpqm/vVi1vapGq2p0ZGRkXjsnSa2ba+i/2A3b0D2/1NXHgZV97VYAh6apS5KGaK6hvxs4PgNnM/Clvvqt3Syeq4FXuuGfh4D1SZZ2H+Cu72qSpCGa8SYqSf4MuBa4OMk4vVk4dwIPJvko8Dxwc9d8D3AjMAa8BnwEoKqOJPk08FjX7lNVdeKHw5KkU2zG0K+qTVOs+sAkbQu4bYr97AB2zKp3kqR55TdyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIacVOgneTbJd5McSLKvq12UZG+Sp7vnpV09Se5OMpbk8SRXzscJSJIGNx9X+r9cVWurarR7vRV4uKpWAw93rwFuAFZ3jy3AvfNwbEnSLJyK4Z2NwM5ueSdwU1/9/ur5JnBhkmWn4PiSpCmcbOgX8FdJ9ifZ0tUuqarDAN3zu7r6cuCFvm3Hu5okaUjecZLbX1NVh5K8C9ib5HvTtM0ktXpbo96bxxaASy+99CS7J0nqd1JX+lV1qHt+CfgisA548fiwTff8Utd8HFjZt/kK4NAk+9xeVaNVNToyMnIy3ZMknWDOoZ/kHya54PgysB54AtgNbO6abQa+1C3vBm7tZvFcDbxyfBhIkjQcJzO8cwnwxSTH97Orqv4yyWPAg0k+CjwP3Ny13wPcCIwBrwEfOYljS5LmYM6hX1XPAO+dpP63wAcmqRdw21yPJ0k6eX4jV5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSEne+csqTndz4n3lj/Te+79iKx0+vNKX5qF/sAfpC6dbrzSl5if0B5kH/5FoIVm6EsMHsbTBbuBrjOBwzuS1BBDX5IaYuhLUkOGHvpJNiT5fpKxJFuHfXxJatlQQz/JIuAe4AbgCmBTkiuG2QdJatmwr/TXAWNV9UxVHQUeADYOuQ+S1Kxhh/5y4IW+1+Nd7e8k2ZJkX5J9ExMTQ+2cNJNdu3bNqi6dboYd+pNNcn7L5Oaq2l5Vo1U1OjIyMqRuSYPZtGkTu3btYs2aNZxzzjmsWbOGXbt2sWnTpoXumjSQYX85axxY2fd6BXBoyH2QTsqmTZsMeZ2xhn2l/xiwOsnlSc4FbgF2D7kPktSsoV7pV9WbSW4HHgIWATuq6uAw+yBJLRv6b+9U1R5gz7CPK0nyG7mS1BRDX5IaktP552CTTADPLXQ/pClcDPxwoTshTeKyqpp0zvtpHfrS6SzJvqoaXeh+SLPh8I4kNcTQl6SGGPrS3G1f6A5Is+WYviQ1xCt9SWqIoS9JDTH0Jakhhr7OOkmOJTnQ95jzvZiT/K/57Fu3z99N8r0kTyT5TpJbu/riJHcmebpb92iSG+b7+Grb0H9wTRqCn1bV2vnYUVX98/nYz3FJfgv4VWBdVf04yTuBm7rVnwaWAe+pqteTXAL8y/k8vuTsHZ11krxaVUsmqT8L7AR+DVgM3FxV30syAuwCfo7ePR82AFdV1Q+P7yvJtcAn6P3swnuA/cBvVFUluQr4LLCkW/9vq+rwFH17HvjlqvrrE+r/gN6tRC+vqh+f7H8DaSoO7+hsdP4Jwzu/3rfuh1V1JXAv8LtdbRvwSFf/InDpFPt9H/Ax4Arg54FrkiwG/hvwoaq6CtgB3DHZxkkuAC44MfA7vwg8b+DrVHN4R2ej6YZ3vtA97wf+dbf8L4APAlTVXyZ5eYptH62qcYAkB4BVwI/oXfnvTQK9mwNNepVP7x7R/mmtBWXoqzWvd8/H+Pt//5nltv3bBzhYVb8008bdGP7/SfLzVfXMCavHgEuTXFBVPxmwP9KsObwjwTeAfwOQZD2wdBbbfh8YSfJL3faLk6yZpv1/Bu5J8rNd+59NsqWqXgPuA+7u7h9NkmVJfmP2pyNNzdDX2ejEMf07Z2j/SWB9km8DN9AbnhnoaruqjgIfAj6T5DvAAWC6GT/3Al8FHkvyBPA/gde6df8RmACe7Nb99+61NG+cvaPmJTkPOFZVb3ZX7PfO15RP6XTjmL7Um63zYJJzgKPAv1vg/kinjFf60imQ5B7gmhPK/7Wq/ngh+iMdZ+hLUkP8IFeSGmLoS1JDDH1JaoihL0kN+f+1H7c4Yw9segAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "road_accidents['Engine_CC'].plot(kind = 'box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: There are outliers in the engine_cc predictor variable. Therefore convert it to categorical variable with 6 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    75550.000000\n",
       "mean      1571.622144\n",
       "std        591.044815\n",
       "min         38.000000\n",
       "25%       1242.000000\n",
       "50%       1596.000000\n",
       "75%       1975.000000\n",
       "max       3500.000000\n",
       "Name: Engine_CC, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_accidents['Engine_CC'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3074.5\n",
      "142.5\n"
     ]
    }
   ],
   "source": [
    "# calculate inter quartile range for Engine_CC\n",
    "IQR = road_accidents['Engine_CC'].quantile(0.75) - road_accidents['Engine_CC'].quantile(0.25)\n",
    "\n",
    "# calculate fences to detect outliers in Engine_CC predictor\n",
    "fence1 = (1.5*IQR)+ road_accidents['Engine_CC'].quantile(0.75) # formula to find fences\n",
    "fence2 = road_accidents['Engine_CC'].quantile(0.25)- (1.5*IQR)\n",
    "\n",
    "print(fence1)\n",
    "print(fence2)\n",
    "\n",
    "# create categorical variable for Engine_CC\n",
    "road_accidents['Engine_CC_cat'] = pd.cut(road_accidents['Engine_CC'], \n",
    "                                         bins = [0,142.5,1242,1596, 1975, 3074.5, 6599], # 6599cc is maximum available vehicle cc\n",
    "                                         labels = ['CC1','CC2','CC3','CC4','CC5','CC6'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the features are already engineered for latitude and longitude as location details and datetime as temporal, they are irrelevant and can be dropped. Also, Accident_Index to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete original Age_of_Vehicle and Hour_of_Day, 'Engine_CC' variables from data and unnecessary columns\n",
    "road_accidents.drop(['Age_of_Vehicle','Hour_of_Day','Engine_CC',\n",
    "                     'Accident_Index', 'Latitude', 'Longitude','Datetime'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract X and y\n",
    "X = road_accidents.copy()\n",
    "del X['Accident_Severity']\n",
    "y=road_accidents['Accident_Severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratefied K-fold sampling to create training and testing set\n",
    "\n",
    "Apply Train Test split with StratifiedShuffleSplit with 50-50% (same split is followed throughout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split with stratified split 50-50%\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=1)\n",
    "sss.get_n_splits(X, y)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dimension Reduction\n",
    "There are 6 numerical and 22 categorical features post preprosessing steps. Therefore, for all classifiers two approaches were explored and best results from feature selection/reduction implemented.\n",
    "1. Use **Random Forest** for feature selection (with variability of approximately 95%).\n",
    "2. Use **MCA** as a feature reduction technique, because of majority categorical features, to transform the categorical data to numerical data (n_components = 22).\n",
    "\n",
    "Upon implementing various n_compenents(range 5-22), n_components = 22 gave overall best results. \n",
    "\n",
    "We demonstrate MCA dimension reduction approach\n",
    "###  MCA approach on categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for seperating data types (numerical and non numerical)\n",
    "'''\n",
    "This function will separate the data into numerical and categorical/object type\n",
    "'''\n",
    "def separate(data):\n",
    "    num = data.dtypes =='int64'\n",
    "    cat = data.dtypes != 'int64'\n",
    "\n",
    "    num = data[data.columns[num]]\n",
    "    cat = data[data.columns[cat]]\n",
    "    \n",
    "    return num, cat\n",
    "\n",
    "'''\n",
    "This function will match the categorical features in the training set and testing set\n",
    "Returns two cleaned data sets\n",
    "'''\n",
    "def cat_feat(train, test):\n",
    "    # Make sure categorical features in both sets match\n",
    "    # Make sure the training feature and testing feature has same number of levels\n",
    "    keep = train.nunique() == test.nunique()\n",
    "    train = train[train.columns[keep]]\n",
    "    test = test[test.columns[keep]]\n",
    "\n",
    "    # Make sure the levels are the same\n",
    "    keep = []\n",
    "    for i in range(train.shape[1]):\n",
    "        keep.append(all(np.sort(train.iloc[:,i].unique()) == np.sort(test.iloc[:,i].unique())))\n",
    "    train = train[train.columns[keep]]\n",
    "    test = test[test.columns[keep]]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical features\n",
    "X_train_num, X_train_cat = separate(X_train)\n",
    "X_test_num, X_test_cat = separate(X_test)\n",
    "\n",
    "# Make sure categorical features match between training and validation sets\n",
    "X_train_cat, X_test_cat = cat_feat(X_train_cat, X_test_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mca on categorical and then combine with numerical data\n",
    "\n",
    "mca = prince.MCA(n_components = 22, random_state = 1) # Instantiate MCA\n",
    "\n",
    "# On train set\n",
    "X_train_cat_fs = mca.fit_transform(X_train_cat) # Fit and Transform on categorical train data\n",
    "X_train_cat_fs = X_train_cat_fs.add_prefix('mca') # Add a prefix to reduced columns\n",
    "X_train_fs = pd.concat([X_train_num, X_train_cat_fs], axis = 1) # Join numerical and reduced columns\n",
    "\n",
    "# On test set\n",
    "X_test_cat_fs = mca.transform(X_test_cat) # Transform on categorical test data\n",
    "X_test_cat_fs = X_test_cat_fs.add_prefix('mca') # Add a prefix to reduced columns\n",
    "X_test_fs = pd.concat([X_test_num, X_test_cat_fs], axis = 1) # Join numerical and reduced columns\n",
    "\n",
    "y_train_fs = y_train.copy()\n",
    "y_test_fs = y_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Modeling\n",
    "\n",
    "### Steps followed for all classifiers:\n",
    "On MCA reduced dataset:\n",
    "1. Setup Hyperparameter tuning \n",
    "2. Use 5 fold gridsearch CV to train and tune the data with classifier\n",
    "3. Obtain best parameters on training and accuracy score on test set\n",
    "4. On exploring certain threshold values for prediction, best accuracy was found at 0.55 for all classifiers. Therefore, same threshold is followed throughout.\n",
    "\n",
    "### Classifier 1: Naive Bayes\n",
    "\n",
    "#### Approach:\n",
    "There are 6 numerical and 22 categorical features finally after preprocessing steps. Upon diagnosing the distribution of numerical features through a histogram plot, it shows they are multinomial and also by applying log transformation (and square transformation) it did not result in a normal distribution. Therefore, the numerical features are multinomial and we keep those features as is.  \n",
    "Further for Naive Bayes classifier two approaches were explored and implemented.\n",
    "1. Taking **all the categorical data** and perform prediction with **CategoricalNB** since majority data is categorical. (Accuracy: 0.7561)\n",
    "2. Use **MCA** as a feature reduction technique (n_components = 22) to transform the categorical data to numerical and perform **GaussianNB** on entire dataset consisting of 6 numerical features and 22 mca transformed features. (Accuracy: 0.7556)\n",
    "\n",
    "We demonstrate approach 2 here for consistency in preprocessing/feature engineering steps as the accuracy results are comparable.\n",
    "\n",
    "#### Explorations on Naive Bayes:\n",
    "1. For MCA n_components range tried (5-22). \n",
    "2. MCA with log transformation on features did not give better results.\n",
    "3. CategoricalNB taking multinomial features as well and encoding them along with categorical features did not give better results.\n",
    "4. RF feature selected GaussianNB accuracy 0.7497, multinomialNB accuracy 0.7259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'priors': [0.9, 0.1], 'var_smoothing': 1e-05}\n",
      "[[  583  8840]\n",
      " [  389 27963]]\n",
      "Accuracy on GaussianNB with threshold 0.55: 0.7556849768365321\n"
     ]
    }
   ],
   "source": [
    "#   ******** Model 1 : Classifier Naive Bayes with GaussianNB ******** \n",
    "# GaussianNB on MCA reduced dataset\n",
    "\n",
    "# Hyperparameter tuning with accuracy as scoring\n",
    "paras = {'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 1e-01, 1],\n",
    "        'priors':[None,[0.1,0.9],[0.2,0.8],[0.3,0.7],[0.9,0.1],[0.8,0.2],[0.7,0.3]]} \n",
    "\n",
    "# Instantiate Gridsearch CV\n",
    "GB_grid = GridSearchCV(GaussianNB(), paras, scoring = 'accuracy', cv = 2, n_jobs = -2)\n",
    "\n",
    "# Fit the model\n",
    "GB_grid.fit(X_train_fs, y_train_fs)\n",
    "\n",
    "# Get the result\n",
    "print(\"Best Params:\",GB_grid.best_params_)\n",
    "\n",
    "# Evaluation on test set with threshold 0.55\n",
    "threshold=0.55\n",
    "y_hat = np.where(GB_grid.predict_proba(X_test_fs)[:,1] > threshold, 1, 0) # Get the predicted classes\n",
    "conf_matrix = confusion_matrix(y_test_fs, y_hat, labels=[1,0])\n",
    "print(conf_matrix)\n",
    "print('Accuracy on GaussianNB with threshold 0.55:', np.mean(y_hat == y_test_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 2: Neural Network\n",
    "\n",
    "Implemented shallow and deep neural network. Shallow network consistently gave better accuracy score\n",
    "\n",
    "#### Explorations on NN:\n",
    "1. RF feature selection with shallow NN accuracy: 0.7599  \n",
    "2. RF feature selection with deep NN accuracy: 0.7505  \n",
    "3. MCA dim reduction with deep NN accuracy: 0.7505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Valid split with stratified split. Valid set to be used in Neural network model for validation\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=1)\n",
    "sss.get_n_splits(X_train_fs, y_train_fs)\n",
    "\n",
    "for train_index, valid_index in sss.split(X_train_fs, y_train_fs):\n",
    "    X_train1_fs, X_valid_fs = X_train_fs.iloc[train_index], X_train_fs.iloc[valid_index]\n",
    "    y_train1_fs, y_valid_fs = y_train_fs.iloc[train_index], y_train_fs.iloc[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "scaler= StandardScaler()\n",
    "X_train1_fs = scaler.fit_transform(X_train1_fs)\n",
    "X_valid_fs = scaler.transform(X_valid_fs)\n",
    "X_test_fs = scaler.transform(X_test_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/30\n",
      "525/591 [=========================>....] - ETA: 0s - loss: 0.6850 - accuracy: 0.6840WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.6768 - accuracy: 0.6913 - val_loss: 0.6104 - val_accuracy: 0.7482\n",
      "Epoch 2/30\n",
      "521/591 [=========================>....] - ETA: 0s - loss: 0.5972 - accuracy: 0.7503WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 975us/step - loss: 0.5970 - accuracy: 0.7500 - val_loss: 0.5861 - val_accuracy: 0.7501\n",
      "Epoch 3/30\n",
      "511/591 [========================>.....] - ETA: 0s - loss: 0.5792 - accuracy: 0.7524WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 991us/step - loss: 0.5795 - accuracy: 0.7506 - val_loss: 0.5722 - val_accuracy: 0.7510\n",
      "Epoch 4/30\n",
      "501/591 [========================>.....] - ETA: 0s - loss: 0.5711 - accuracy: 0.7498WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5678 - accuracy: 0.7519 - val_loss: 0.5623 - val_accuracy: 0.7524\n",
      "Epoch 5/30\n",
      "523/591 [=========================>....] - ETA: 0s - loss: 0.5583 - accuracy: 0.7547WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 982us/step - loss: 0.5589 - accuracy: 0.7536 - val_loss: 0.5546 - val_accuracy: 0.7538\n",
      "Epoch 6/30\n",
      "587/591 [============================>.] - ETA: 0s - loss: 0.5516 - accuracy: 0.7542WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 990us/step - loss: 0.5519 - accuracy: 0.7539 - val_loss: 0.5486 - val_accuracy: 0.7549\n",
      "Epoch 7/30\n",
      "524/591 [=========================>....] - ETA: 0s - loss: 0.5486 - accuracy: 0.7538WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 977us/step - loss: 0.5467 - accuracy: 0.7551 - val_loss: 0.5440 - val_accuracy: 0.7554\n",
      "Epoch 8/30\n",
      "582/591 [============================>.] - ETA: 0s - loss: 0.5419 - accuracy: 0.7566WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5425 - accuracy: 0.7561 - val_loss: 0.5404 - val_accuracy: 0.7566\n",
      "Epoch 9/30\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7573WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5392 - accuracy: 0.7573 - val_loss: 0.5375 - val_accuracy: 0.7570\n",
      "Epoch 10/30\n",
      "586/591 [============================>.] - ETA: 0s - loss: 0.5373 - accuracy: 0.7570WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 990us/step - loss: 0.5367 - accuracy: 0.7576 - val_loss: 0.5354 - val_accuracy: 0.7578\n",
      "Epoch 11/30\n",
      "504/591 [========================>.....] - ETA: 0s - loss: 0.5346 - accuracy: 0.7586WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 998us/step - loss: 0.5349 - accuracy: 0.7578 - val_loss: 0.5337 - val_accuracy: 0.7586\n",
      "Epoch 12/30\n",
      "568/591 [===========================>..] - ETA: 0s - loss: 0.5349 - accuracy: 0.7566WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5335 - accuracy: 0.7575 - val_loss: 0.5325 - val_accuracy: 0.7580\n",
      "Epoch 13/30\n",
      "493/591 [========================>.....] - ETA: 0s - loss: 0.5348 - accuracy: 0.7564WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 986us/step - loss: 0.5325 - accuracy: 0.7579 - val_loss: 0.5313 - val_accuracy: 0.7593\n",
      "Epoch 14/30\n",
      "584/591 [============================>.] - ETA: 0s - loss: 0.5319 - accuracy: 0.7577WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5317 - accuracy: 0.7577 - val_loss: 0.5307 - val_accuracy: 0.7591\n",
      "Epoch 15/30\n",
      "585/591 [============================>.] - ETA: 0s - loss: 0.5316 - accuracy: 0.7577WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5311 - accuracy: 0.7582 - val_loss: 0.5298 - val_accuracy: 0.7598\n",
      "Epoch 16/30\n",
      "563/591 [===========================>..] - ETA: 0s - loss: 0.5313 - accuracy: 0.7577WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7585 - val_loss: 0.5293 - val_accuracy: 0.7592\n",
      "Epoch 17/30\n",
      "507/591 [========================>.....] - ETA: 0s - loss: 0.5300 - accuracy: 0.7579WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5302 - accuracy: 0.7581 - val_loss: 0.5287 - val_accuracy: 0.7594\n",
      "Epoch 18/30\n",
      "529/591 [=========================>....] - ETA: 0s - loss: 0.5293 - accuracy: 0.7587WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 982us/step - loss: 0.5299 - accuracy: 0.7582 - val_loss: 0.5284 - val_accuracy: 0.7593\n",
      "Epoch 19/30\n",
      "581/591 [============================>.] - ETA: 0s - loss: 0.5294 - accuracy: 0.7583WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5296 - accuracy: 0.7582 - val_loss: 0.5281 - val_accuracy: 0.7594\n",
      "Epoch 20/30\n",
      "523/591 [=========================>....] - ETA: 0s - loss: 0.5289 - accuracy: 0.7577WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5294 - accuracy: 0.7581 - val_loss: 0.5279 - val_accuracy: 0.7592\n",
      "Epoch 21/30\n",
      "589/591 [============================>.] - ETA: 0s - loss: 0.5291 - accuracy: 0.7588WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7587 - val_loss: 0.5276 - val_accuracy: 0.7597\n",
      "Epoch 22/30\n",
      "570/591 [===========================>..] - ETA: 0s - loss: 0.5296 - accuracy: 0.7572WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 1ms/step - loss: 0.5289 - accuracy: 0.7578 - val_loss: 0.5274 - val_accuracy: 0.7593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30\n",
      "547/591 [==========================>...] - ETA: 0s - loss: 0.5299 - accuracy: 0.7575WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 963us/step - loss: 0.5288 - accuracy: 0.7582 - val_loss: 0.5274 - val_accuracy: 0.7593\n",
      "Epoch 24/30\n",
      "538/591 [==========================>...] - ETA: 0s - loss: 0.5281 - accuracy: 0.7595WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 972us/step - loss: 0.5286 - accuracy: 0.7587 - val_loss: 0.5271 - val_accuracy: 0.7593\n",
      "Epoch 25/30\n",
      "527/591 [=========================>....] - ETA: 0s - loss: 0.5302 - accuracy: 0.7569WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 989us/step - loss: 0.5285 - accuracy: 0.7580 - val_loss: 0.5268 - val_accuracy: 0.7597\n",
      "Epoch 26/30\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.5276 - accuracy: 0.7583WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 960us/step - loss: 0.5283 - accuracy: 0.7584 - val_loss: 0.5271 - val_accuracy: 0.7591\n",
      "Epoch 27/30\n",
      "534/591 [==========================>...] - ETA: 0s - loss: 0.5289 - accuracy: 0.7577WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 963us/step - loss: 0.5282 - accuracy: 0.7585 - val_loss: 0.5263 - val_accuracy: 0.7594\n",
      "Epoch 28/30\n",
      "505/591 [========================>.....] - ETA: 0s - loss: 0.5285 - accuracy: 0.7580WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 994us/step - loss: 0.5280 - accuracy: 0.7578 - val_loss: 0.5267 - val_accuracy: 0.7589\n",
      "Epoch 29/30\n",
      "539/591 [==========================>...] - ETA: 0s - loss: 0.5267 - accuracy: 0.7588WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 975us/step - loss: 0.5279 - accuracy: 0.7581 - val_loss: 0.5259 - val_accuracy: 0.7586\n",
      "Epoch 30/30\n",
      "526/591 [=========================>....] - ETA: 0s - loss: 0.5293 - accuracy: 0.7568WARNING:tensorflow:Early stopping conditioned on metric `Val_loss` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "591/591 [==============================] - 1s 978us/step - loss: 0.5278 - accuracy: 0.7582 - val_loss: 0.5261 - val_accuracy: 0.7582\n",
      "{'activations': 'relu', 'l2_penalty': 0.01, 'n_neurons': 4}\n",
      "WARNING:tensorflow:From C:\\Users\\Swati\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:264: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "accuracy with threshold 0.5 is: 0.7575380542686962\n"
     ]
    }
   ],
   "source": [
    "#   ******** Model 2 : Classifier NN ******** \n",
    "# Shallow neural network on MCA reduced dataset\n",
    "set_seed(1)\n",
    "seed(1)\n",
    "# setup hyper parameter set\n",
    "param = {'n_neurons':[4],\n",
    "        'activations':['relu'],\n",
    "        'l2_penalty':[0.01]}\n",
    "\n",
    "n_input = X_train1_fs.shape[1]\n",
    "optimizer = 'SGD'\n",
    "\n",
    "# create a wrapper function\n",
    "def tuning_model(n_neurons, activations, l2_penalty):\n",
    "    model3 = Sequential() # Instantiate the model\n",
    "    options = {\"input_dim\": n_input} # Set options \n",
    "    print(n_neurons)\n",
    "    model3.add(Dense(n_neurons, activation = activations, **options, kernel_regularizer = l2(l2_penalty))) # Using the input options from before\n",
    "    options = {} # Erase the input options so it won't be included in future layers\n",
    "    model3.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model3.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics=['accuracy'])\n",
    "    return model3\n",
    "\n",
    "# Set up the grid search\n",
    "keras_clf = tf.keras.wrappers.scikit_learn.KerasClassifier(tuning_model) \n",
    "\n",
    "grd_cv = GridSearchCV(keras_clf, param, cv = 2, n_jobs = -2)\n",
    "\n",
    "# Fit the model\n",
    "grd_cv.fit(X_train1_fs, y_train1_fs, epochs = 30, batch_size=32,\n",
    "          validation_data = (X_valid_fs, y_valid_fs),\n",
    "          callbacks = tf.keras.callbacks.EarlyStopping(monitor='Val_loss', patience=3)) #Set early stopping criteria\n",
    "\n",
    "print(grd_cv.best_params_)\n",
    "threshold=0.5\n",
    "y_hat = np.where(grd_cv.predict_proba(X_test_fs)[:,1] > threshold, 1, 0) # Get the predicted classes\n",
    "print('accuracy with threshold 0.5 is:', np.mean(y_hat == y_test_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#   ******** Model 2 : Classifier NN ******** \\n# Shallow neural network on MCA reduced dataset\\nset_seed(1)\\nseed(1)\\n# setup hyper parameter set\\nparam = {\\'n_neurons\\':[4,5,7,10],\\n        \\'activations\\':[\\'relu\\'],\\n        \\'l2_penalty\\':[0.1, 0.01]}\\n\\nn_input = X_train1_fs.shape[1]\\noptimizer = \\'SGD\\'\\n\\n# create a wrapper function\\ndef tuning_model(n_neurons, activations, l2_penalty):\\n    model3 = Sequential() # Instantiate the model\\n    options = {\"input_dim\": n_input} # Set options \\n    print(n_neurons)\\n    model3.add(Dense(n_neurons, activation = activations, **options, kernel_regularizer = l2(l2_penalty))) # Using the input options from before\\n    options = {} # Erase the input options so it won\\'t be included in future layers\\n    model3.add(Dense(1, activation = \\'sigmoid\\'))\\n    \\n    # compile the model\\n    model3.compile(loss = \\'binary_crossentropy\\', optimizer = \\'SGD\\', metrics=[\\'accuracy\\'])\\n    return model3\\n\\n# Set up the grid search\\nkeras_clf = tf.keras.wrappers.scikit_learn.KerasClassifier(tuning_model) \\n\\ngrd_cv = GridSearchCV(keras_clf, param, cv = 2, n_jobs = -2)\\n\\n# Fit the model\\ngrd_cv.fit(X_train1_fs, y_train1_fs, epochs = 30, batch_size=32,\\n          validation_data = (X_valid_fs, y_valid_fs),\\n          callbacks = tf.keras.callbacks.EarlyStopping(monitor=\\'Val_loss\\', patience=3)) #Set early stopping criteria\\n\\nprint(grd_cv.best_params_)\\nthreshold=0.55\\ny_hat = np.where(grd_cv.predict_proba(X_test_fs)[:,1] > threshold, 1, 0) # Get the predicted classes\\nprint(\\'accuracy with threshold 0.55 is:\\', np.mean(y_hat == y_test_fs))\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#   ******** Model 2 : Classifier NN ******** \n",
    "# Shallow neural network on MCA reduced dataset\n",
    "set_seed(1)\n",
    "seed(1)\n",
    "# setup hyper parameter set\n",
    "param = {'n_neurons':[4,5,7,10],\n",
    "        'activations':['relu'],\n",
    "        'l2_penalty':[0.1, 0.01]}\n",
    "\n",
    "n_input = X_train1_fs.shape[1]\n",
    "optimizer = 'SGD'\n",
    "\n",
    "# create a wrapper function\n",
    "def tuning_model(n_neurons, activations, l2_penalty):\n",
    "    model3 = Sequential() # Instantiate the model\n",
    "    options = {\"input_dim\": n_input} # Set options \n",
    "    print(n_neurons)\n",
    "    model3.add(Dense(n_neurons, activation = activations, **options, kernel_regularizer = l2(l2_penalty))) # Using the input options from before\n",
    "    options = {} # Erase the input options so it won't be included in future layers\n",
    "    model3.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model3.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics=['accuracy'])\n",
    "    return model3\n",
    "\n",
    "# Set up the grid search\n",
    "keras_clf = tf.keras.wrappers.scikit_learn.KerasClassifier(tuning_model) \n",
    "\n",
    "grd_cv = GridSearchCV(keras_clf, param, cv = 2, n_jobs = -2)\n",
    "\n",
    "# Fit the model\n",
    "grd_cv.fit(X_train1_fs, y_train1_fs, epochs = 30, batch_size=32,\n",
    "          validation_data = (X_valid_fs, y_valid_fs),\n",
    "          callbacks = tf.keras.callbacks.EarlyStopping(monitor='Val_loss', patience=3)) #Set early stopping criteria\n",
    "\n",
    "print(grd_cv.best_params_)\n",
    "threshold=0.55\n",
    "y_hat = np.where(grd_cv.predict_proba(X_test_fs)[:,1] > threshold, 1, 0) # Get the predicted classes\n",
    "print('accuracy with threshold 0.55 is:', np.mean(y_hat == y_test_fs))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 3: SVM\n",
    "Implemented LinearSVC, SVC with kernel poly and rbf. \n",
    "SVC with kernel poly consistently gave better results across all three types of SVC classifiers.\n",
    "\n",
    "#### Explorations on SVM:\n",
    "1. For MCA n_components range tried (5-22). \n",
    "2. SVC poly on RF feature selected and threshold 0.55 accuracy: 0.7601 (conventional train test split method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'poly__C': 0.1, 'poly__coef0': 2, 'poly__degree': 3}\n",
      "Accuracy on SVC poly with threshold 0.55: 0.750549305095963\n"
     ]
    }
   ],
   "source": [
    "#   ******** Model 3 : Classifier SVC with kernel poly ******** \n",
    "# SVC kernel poly on MCA reduced dataset\n",
    "\n",
    "# Setup pipeline\n",
    "svm_clf = Pipeline([\n",
    "                    ('scalar', StandardScaler()),\n",
    "                    ('poly', SVC(kernel = 'poly', random_state = 1, probability=True))\n",
    "                    ])\n",
    "\n",
    "# Setup parameters to tune\n",
    "parameters = {'poly__C':[0.1],\n",
    "             'poly__degree':[3],\n",
    "             'poly__coef0':[2]}\n",
    "\n",
    "svm_poly = GridSearchCV(svm_clf, parameters, cv=5, n_jobs = -2)\n",
    "\n",
    "# Fit the model\n",
    "svm_poly.fit(X_train_fs, y_train_fs) \n",
    "\n",
    "print(\"Best Params:\",svm_poly.best_params_)\n",
    "\n",
    "# Evaluation on test set with threshold 0.55\n",
    "threshold=0.55\n",
    "y_hat = np.where(svm_poly.predict_proba(X_test_fs)[:,1] > threshold, 1, 0) # Get the predicted classes\n",
    "print('Accuracy on SVC poly with threshold 0.55:', np.mean(y_hat == y_test_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def process_new_file(X_unseen,X_train_cat ):\n",
    "    \n",
    "# Step 1. Preprocess \n",
    "    # Convert multinomial features to categorical by converting to datatype string\n",
    "    \n",
    "    X_unseen['Season'] = X_unseen['Season'].apply(str)\n",
    "    X_unseen['Month_of_Year'] = X_unseen['Month_of_Year'].apply(str)\n",
    "    X_unseen['Day_of_Week'] = X_unseen['Day_of_Week'].apply(str)\n",
    "\n",
    "    # Create categorical feature from Age_of_Vehicle\n",
    "    X_unseen['Age_of_Vehicle_cat'] = pd.qcut(X_unseen['Age_of_Vehicle'], \n",
    "                                                   [0,0.25,0.5,0.75,1], labels = ['new','average', 'old', 'very_old'])\n",
    "\n",
    "    # Create categorical feature from Hour_of_Day predictors\n",
    "    X_unseen['Hour_of_Day_cat'] = pd.qcut(X_unseen['Hour_of_Day'], \n",
    "            [0,0.25,0.5,0.75,1], labels=['early_morning', 'morning', 'afternoon', 'night'])\n",
    "\n",
    "    X_unseen['Engine_CC_cat'] = pd.cut(X_unseen['Engine_CC'], \n",
    "                                             bins = [0,142.5,1242,1596, 1975, 3074.5, 6599],\n",
    "                                             labels = ['CC1','CC2','CC3','CC4','CC5','CC6'])\n",
    "\n",
    "    # Delete original Age_of_Vehicle, Hour_of_Day, Engine_CC variables and unnecessary columns from data\n",
    "    X_unseen.drop(['Accident_Index', 'Latitude', 'Longitude','Datetime','Age_of_Vehicle','Hour_of_Day', 'Engine_CC'],\n",
    "                  axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2. MCA dim reduction\n",
    "    # Separate numerical and categorical features\n",
    "    X_new_num, X_new_cat = separate(X_unseen)\n",
    "    # Make sure categorical features match between training and validation sets\n",
    "    X_train_cat, X_new_cat = cat_feat(X_train_cat, X_new_cat)\n",
    "    X_new_cat_fs = mca.transform(X_new_cat) # Transform on categorical test data\n",
    "    X_new_cat_fs = X_new_cat_fs.add_prefix('mca') # Add a prefix to reduced columns\n",
    "    X_new_fs = pd.concat([X_new_num, X_new_cat_fs], axis = 1) # Join numerical and reduced columns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Step 3. Prediction on X_test_fs set and return output dataframe\n",
    "    threshold=0.55 # set threshold\n",
    "    \n",
    "    # Classifier 1: Naive Bayes- GaussianNB\n",
    "    y_hat_gb = np.where(GB_grid.predict_proba(X_new_fs)[:,1] > threshold, 1, 0) # Get the predicted classes\n",
    "    \n",
    "    # Classifier 2: Neural Network- shallow \n",
    "    X_new_fs_nn = scaler.transform(X_new_fs)\n",
    "    threshold2 = 0.5\n",
    "    y_hat_nn = np.where(grd_cv.predict_proba(X_new_fs_nn)[:,1] > threshold2, 1, 0) # Get the predicted classes\n",
    "    \n",
    "    # Classifier 3: SVM- kernel poly\n",
    "    y_hat_svm = np.where(svm_poly.predict_proba(X_new_fs)[:,1] > threshold, 1, 0) # Get the predicted classes\n",
    "    \n",
    "    \n",
    "   # Return Dataframe with the predictions\n",
    "    data_features = list(zip(y_hat_svm,y_hat_nn,y_hat_gb))\n",
    "    df = pd.DataFrame(data_features, columns = ['SVM','NN','NB'])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read unseen new file\n",
    "X_new = pd.read_csv('test_small.csv')\n",
    "predicted = process_new_file(X_new.copy(),X_train_cat)\n",
    "\n",
    "# Binary decode target variable of Accident_Severity\n",
    "# 'Fatal_Serious': 1, 'Slight': 0\n",
    "predicted.replace(to_replace = 0, value ='Slight', inplace=True)\n",
    "predicted.replace(to_replace = 1, value ='Fatal_Serious',inplace=True)\n",
    "predicted.insert(0, 'Accident_Index', X_new['Accident_Index']) # Add Accident_Index Column\n",
    "\n",
    "# Export predicted df to csv\n",
    "predicted.to_csv('submission_small.csv', index = False)\n",
    "print(predicted.shape)\n",
    "predicted\n",
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NN</th>\n",
       "      <th>NB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>69996</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>69997</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>69998</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>69999</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>70000</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accident_Index            SVM      NN             NB\n",
       "0                   1         Slight  Slight         Slight\n",
       "1                   2  Fatal_Serious  Slight  Fatal_Serious\n",
       "2                   3         Slight  Slight         Slight\n",
       "3                   4         Slight  Slight         Slight\n",
       "4                   5         Slight  Slight         Slight\n",
       "...               ...            ...     ...            ...\n",
       "69995           69996         Slight  Slight         Slight\n",
       "69996           69997         Slight  Slight         Slight\n",
       "69997           69998         Slight  Slight         Slight\n",
       "69998           69999         Slight  Slight         Slight\n",
       "69999           70000         Slight  Slight         Slight\n",
       "\n",
       "[70000 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slight           67945\n",
       "Fatal_Serious     2055\n",
       "Name: NN, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.NN.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slight           68131\n",
       "Fatal_Serious     1869\n",
       "Name: SVM, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.SVM.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slight           68216\n",
       "Fatal_Serious     1784\n",
       "Name: NB, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.NB.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>SVM</th>\n",
       "      <th>NN</th>\n",
       "      <th>NB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>69996</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>69997</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>69998</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>69999</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>70000</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accident_Index            SVM      NN             NB\n",
       "0                   1         Slight  Slight         Slight\n",
       "1                   2  Fatal_Serious  Slight  Fatal_Serious\n",
       "2                   3         Slight  Slight         Slight\n",
       "3                   4         Slight  Slight         Slight\n",
       "4                   5         Slight  Slight         Slight\n",
       "...               ...            ...     ...            ...\n",
       "69995           69996         Slight  Slight         Slight\n",
       "69996           69997         Slight  Slight         Slight\n",
       "69997           69998         Slight  Slight         Slight\n",
       "69998           69999         Slight  Slight         Slight\n",
       "69999           70000         Slight  Slight         Slight\n",
       "\n",
       "[70000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv('submission_small.csv')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Index</th>\n",
       "      <th>Accident_Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69995</th>\n",
       "      <td>69996</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69996</th>\n",
       "      <td>69997</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>69998</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>69999</td>\n",
       "      <td>Slight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>70000</td>\n",
       "      <td>Fatal_Serious</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accident_Index Accident_Severity\n",
       "0                   1            Slight\n",
       "1                   2     Fatal_Serious\n",
       "2                   3     Fatal_Serious\n",
       "3                   4            Slight\n",
       "4                   5            Slight\n",
       "...               ...               ...\n",
       "69995           69996            Slight\n",
       "69996           69997     Fatal_Serious\n",
       "69997           69998     Fatal_Serious\n",
       "69998           69999            Slight\n",
       "69999           70000     Fatal_Serious\n",
       "\n",
       "[70000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subr = pd.read_csv('test_label_small.csv')\n",
    "subr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Metric\n",
    "1. SVM : 0.7575571  \n",
    "2. Neural Network : 0.7558714  \n",
    "3. Naive Bayes : 0.7527714  \n",
    "\n",
    "**Poly kernel SVC algorithm shows best result overall.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
